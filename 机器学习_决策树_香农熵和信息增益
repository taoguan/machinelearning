http://blog.csdn.net/q1007729991/article/details/40736757

一、如何衡量样本的混乱度？

先来谈谈什么叫样本的乱混度？


假设有一桶鱼，一共就包含了两种鱼，一种是鲫鱼，另一种是鲤鱼，如果说这桶鱼里大部分都是鲫鱼，只有了了几条鲤鱼，那么说，这桶鱼的纯度(purity)是比较高的，反过来说就是混乱度比较低，对应的熵就小（熵就是来描述混乱程度的嘛）。如果说这桶鱼鲫鱼和鲤鱼基本上都差不多数量，那意味着此时纯度就比较低，也即混乱度高，此是对应的熵就大。


如果这桶鱼包含了各种各样的鱼，那纯度肯定会越来越低，熵会越来越大。


于是就引出了一个话题：

熵怎么计算？

了解了基本的概念后，大家可以试着用数字来衡量混乱度的大小。。。貌似比较困难。伟大的信息论之父香农已经为我们做好了这件事情，他给出了下面的公式：



大多数人都是看到公式就头大的，这里用语言描述一下，S代表的是整个集合，也就是这个桶里所有的鱼。pi代表的是第i类所占的比例，m表示一共有m个类别。桶里一共100条鱼，其中39条鲫鱼，61条鲤鱼，即m=2，那鲫鱼所的比例p1就是39/100，鲤鱼占的比例p2就是61/100，信息熵大小就是：

Entropy ([39+, 61-]) = - (39/100) log2 (39/100)-(61/100) log2(61/100) = 0.8931734583778568

上面的记号39+表示鲫鱼数量，61-表示鲤鱼数量，大家不必过分在意符号的表示，你可以选择你自己认为合适的记号，只要可以区分开就可。但是有一套合适的记号系统是一个很好的习惯，这会不仅会提高工作效率，也更容易让人理解。

二、如何去评价一个鱼的某个特征对鱼的区分的好坏
举个简单的例子，假设我问你，鲫鱼和鲤鱼怎么区分，你怎么回答？


你可能会这样回答，有须的是鲤鱼，没有须的就是鲫鱼喽！（我觉得你不会说重一点的是鲤鱼，轻点的是鲫鱼吧。。。这样真的很难区分唉！换句话说这个重量特征对鱼的区分程度简直太低，术语叫信息增益太小）


非常好！假设我这个桶中还混有泥鳅，你上面的回答肯定应该就是这样了：没有须的是鲫鱼喽！因为你知道，这时候桶里还有泥鳅，泥鳅也是有须的嘛！这意味着是否有须对鲫鱼和鲤鱼的区分程度已经相比之前降低了一个档次。


其实上面的例子中，我们已经无形的在使用决策树了！


如何计算某个特征对分类的区分程度?

前面已经不经意提到了信息增益的直观上的含义，那么问题来了^_^问：信息增益要怎么计算？（即某个特征对分类的区分程度）


想想就觉得好难是不是？牛哄哄的香农又给了我们一个公式：




我知道，略晕略晕 -_-#。。。


下面我再一个个解释一下吧：


S代表整个集合，前面的例子中的桶里所有的鱼的集合。

Entropy(S)叫前面已经说过了，集合的混乱度（这桶鱼的混乱程度）。A表示特征（如：是否有须），v表示特征的值（如：有须，没须）。Sv表示特征为A的值为v的子集（如：有须的划分一类，没须的划分为另一类，这里就划分出了两个集合了），Entropy(Sv)就是这个子集Sv的混乱程度（你肯定希望这个Sv越小越好，也就是越不混乱越好，也就是说你希望提出的那个分类标准有须和没须能够最大程度的区分鲫鱼和鲤鱼，这个时候得到的信息增益也是最大的）

体验信息增益的计算
1）按照是否有须来划分
上一节已经计算出来了Entropy(S)= 0.8931734583778568

集合S有须={鲤鱼，鲤鱼，……}，其中v=有须

集合S无须={鲫鱼，鲫鱼，……}，其中v=没须


易知：

|S有须|=61，|S无须|=39

Entropy(S有须)=0

Entropy(S无须)=0



代入公式得：

Gain(S，是否有须) = Entropy(S) – 0 =0.8931734583778568


这特征的信息增益取到了最大的值。

2)按照体重来划分
假设按照是否>0.5kg的标准来算吧-_-#

集合S>0.5kg=｛鲤鱼，鲤鱼，……，鲫鱼，鲫鱼，……｝（鲤鱼：53条；鲫鱼：19条）

集合S≤0.5kg=｛鲤鱼，鲤鱼，……，鲫鱼，鲫鱼，……｝（鲤鱼：8条；鲫鱼：20条）

反正我家的桶里算到这样的结果，你家的你回去数数看。


易知：

|S>0.5kg| = 72

|S≤0.5kg| = 28

Entropy(S>0.5kg) = - (53/72) log2 (53/72)-(19/72) log2(19/72) = 0.8325582396745668

Entropy(S≤0.5kg) = - (8/28) log2 (8/28)-(20/28) log2(20/28) = 0.863120568566631



注意这上面两个结果比较大哦！

最后

Gain(S，是否大于0.5kg) 

=0.8931734583778568-[(72/100)*0. 8325582396745668+(28/100)* 0.863120568566631] 

= 0.052057766613512024



发现这个值远远比上面用是否有须划分的信息增益来的小。这意味着这个按重量来划分是真的不靠谱！


关于香农熵和信息增益就说到这里了！

参考资料：
[1] 机器学习实战 作者：Peter Harrington
[2] 机器学习 作者：Tom M.Mitchell
